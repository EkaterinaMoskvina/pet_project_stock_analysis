# Аналитическая Платформа для Прогнозирования Цен Акций

Этот pet-project представляет собой end-to-end MLOps платформу для сбора, обработки, анализа данных по акциям и прогнозирования их стоимости на неделю вперед. Проект построен на современном стеке технологий, который часто используется в production-окружениях.

## Цель проекта

Создать полностью автоматизированный пайплайн, который:
1.  **Собирает** исторические данные по котировкам акций из публичного API.
2.  **Хранит** сырые данные в Data Lake (S3) и обработанные данные в аналитическом хранилище (ClickHouse).
3.  **Трансформирует** и подготавливает данные для обучения моделей машинного обучения.
4.  **Обучает** ML-модели для прогнозирования цен и сохраняет их артефакты.
5.  **Делает прогнозы** на основе свежих данных и сохраняет результаты.
6.  **Визуализирует** исторические данные и прогнозы на дашбордах.

## Технологический стек

| Инструмент | Категория | Роль в проекте |
| :--- | :--- | :--- |
| **Python** | Язык | Основной язык для написания скриптов и логики. |
| **Docker** | Контейнеризация | Изолированное развертывание всех сервисов локально. |
| **Airflow** | Оркестратор | Управление и запуск всех задач по расписанию. |
| **`dlt`** | Загрузчик (EL) | Загрузка данных из API в Data Lake. |
| **S3 (MinIO)** | Data Lake | Хранилище для сырых данных, моделей и других артефактов. |
| **ClickHouse** | DWH | Сверхбыстрая аналитическая база данных для витрин. |
| **dbt** | Трансформер (T) | Преобразование данных внутри ClickHouse (EL**T**-подход). |
| **DuckDB** | Ad-hoc аналитика | Быстрый анализ данных напрямую в S3. |
| **MLflow** | MLOps | Отслеживание экспериментов, версионирование моделей. |
| **Scikit-learn** | Машинное обучение | Библиотека для создания ML моделей. |
| **Apache Superset** | BI / Визуализация | Создание интерактивных дашбордов. |
| **Metastore** | Каталог данных | Хранение метаданных о схемах данных в S3. |

## 🏗️ Архитектура и поток данных (Data Flow)

Процесс разделен на четыре основных этапа, которые оркестрируются с помощью Airflow.
**Этап 1: Сбор и хранение сырых данных (Ingestion & Data Lake)**
- Оркестратор (Airflow) по расписанию (например, раз в день) запускает задачу (DAG).
- Скрипт на Python + dlt обращается к API фондового рынка (Yahoo Finance). dlt загружает полученные сырые данные (например, в формате JSON или CSV) в S3 (Data Lake). Структура в S3 может быть партиционирована по дате, например: s3://my-stock-bucket/raw/yyyy/mm/dd/data.parquet.
**Этап 2: Трансформация и загрузка в аналитическую базу (ELT & DWH)**
4. После успешной загрузки в S3, Airflow запускает следующую задачу.
5. Эта задача использует dbt (Data Build Tool) для трансформации данных. dbt подключается к ClickHouse, читает сырые данные из S3 (ClickHouse умеет это делать напрямую!), очищает их, приводит к нужным типам, обогащает и складывает в витрины данных (Data Marts) уже внутри ClickHouse.
6. DuckDB здесь может использоваться как вспомогательный инструмент для быстрых локальных тестов или для ad-hoc анализа данных прямо в S3, минуя ClickHouse.
7. Metastore (например, AWS Glue Catalog) используется для того, чтобы дать "схему" и структуру файлам в нашем Data Lake (S3). Это позволяет таким инструментам как ClickHouse или DuckDB видеть файлы в S3 не как набор файлов, а как структурированную таблицу.
**Этап 3: Обучение и использование моделей ML (Machine Learning)**
8. Airflow запускает скрипт обучения модели.
9. Скрипт (на Python с использованием scikit-learn, pandas, pytorch и т.д.) забирает подготовленные данные из витрин в ClickHouse.
10. Происходит обучение модели (например, предсказание цены на неделю вперед). Обученная модель и ее метрики сохраняются с помощью MLflow, а сама модель (артефакт) — в S3.
11. Airflow по другому расписанию (например, раз в неделю) запускает скрипт для инференса (прогнозирования).
12. Этот скрипт загружает последнюю версию модели из MLflow/S3, берет свежие данные из ClickHouse и делает прогноз.
13. Результаты прогноза сохраняются обратно в ClickHouse в отдельную таблицу.
**Этап 4: Визуализация (Analytics & BI)**
14. BI-инструмент (например, Apache Superset, Metabase или Grafana) подключается к ClickHouse.
15. В нем строятся дашборды, которые показывают:
* Исторические данные по акциям.
* Результаты прогнозов.
* Точность предыдущих прогнозов.
* Другие аналитические срезы.



```mermaid
graph TD
    subgraph "Этап 1: Сбор данных (Ingestion)"
        A[API Источник] -->|1. Запрос данных| B(Python + dlt);
        B -->|2. Загрузка сырых данных| C[S3 Data Lake];
    end

    subgraph "Этап 2: Трансформация (ELT)"
        C -->|3. Чтение из S3| D{dbt};
        D -->|4. Трансформация и запись| E[(ClickHouse DWH)];
    end

    subgraph "Этап 3: Машинное обучение (ML)"
        E -->|5. Данные для обучения| F[ML Model Training];
        F -->|6. Логирование эксперимента| G[MLflow];
        F -->|7. Сохранение модели| C;
        E -->|8. Свежие данные| H[ML Inference];
        C -->|Загрузка модели| H;
        H -->|9. Запись прогноза| E;
    end

    subgraph "Эта- 4: Визуализация (Analytics)"
        E -->|10. Данные для отчетов| I(BI-инструмент / Superset);
        I --> J[Дашборды];
    end

    subgraph "Оркестрация"
        K(Airflow) -.-> B;
        K -.-> D;
        K -.-> F;
        K -.-> H;
    end
